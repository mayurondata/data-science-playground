{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e571ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Web Scraping\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites. Unlike web crawling, which focuses on discovering and listing URLs, web scraping involves **accessing the content within each page and pulling out specific pieces of information** — such as titles, prices, reviews, dates, or any structured content.\n",
    "\n",
    "\n",
    "\n",
    "Web scraping typically involves:\n",
    "1. **Sending HTTP requests** to web pages\n",
    "2. **Parsing the HTML** of the response\n",
    "3. **Extracting target data** using selectors (CSS, XPath, etc.)\n",
    "4. **Structuring and storing the data** in formats like JSON, CSV, or databases\n",
    "\n",
    "Scraping can be done using tools like Python (with libraries such as Scrapy, BeautifulSoup, Selenium)\n",
    "> Note: Always check a website’s Terms of Service and robots.txt file before scraping. Respect ethical and legal boundaries when collecting data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed8e46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Common Use Cases of Web Scraping\n",
    "\n",
    "- **Price Monitoring**  \n",
    "  Websites that track prices across e-commerce sites use scrapers to collect prices daily or hourly.\n",
    "\n",
    "- **Job Aggregators**  \n",
    "  Platforms like Indeed or Glassdoor scrape job listings from multiple company websites.\n",
    "\n",
    "- **Market Research**  \n",
    "  Scrapers collect customer reviews, product specs, or competitor data to support business decisions.\n",
    "\n",
    "- **Academic or Data Projects**  \n",
    "  Researchers and students scrape data for machine learning, NLP, or data analysis tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Real-Life Analogy: The Data Collector\n",
    "\n",
    "Let’s go back to our **library analogy**.\n",
    "\n",
    "- A **web scraper** is like a **researcher** who walks up to the books listed by the scout (crawler).\n",
    "- This researcher **opens the books**, **reads specific pages**, and **copies down key facts** — like author names, chapters, or quotes.\n",
    "- So, while the crawler builds the roadmap, the scraper collects the actual **content**.\n",
    "\n",
    "---\n",
    "\n",
    "### Popular Tools for Web Scraping\n",
    "\n",
    "| Tool                        | Category                | Notes                                             |\n",
    "| --------------------------- | ----------------------- | ------------------------------------------------ |\n",
    "| **BeautifulSoup**           | Scraper                 | Great for parsing HTML and extracting data        |\n",
    "| **Selenium**                | Scraper                 | Handles JavaScript-rendered pages with automation |\n",
    "| **Scrapy**                  | Crawler + Scraper       | End-to-end framework for crawling and scraping    |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Handling Loosely Structured HTML with Relative Navigation\n",
    "\n",
    "Sometimes, elements in HTML don’t have classes or IDs. In such cases, we use relative navigation.\n",
    "\n",
    "#### Relative Navigation in Web Scraping\n",
    "\n",
    "When scraping data, we often start from a known HTML tag (like a heading) and navigate to nearby elements such as paragraphs or lists using their position in the document.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "heading = soup.find('h2', string='Some Title')\n",
    "paragraph = heading.find_next_sibling('p')\n",
    "list_section = paragraph.find_next_sibling('ul')\n",
    "```\n",
    "\n",
    "This method is helpful when content is structured visually but not clearly marked in the HTML.\n",
    "\n",
    "**Common methods in BeautifulSoup:**\n",
    "\n",
    "| Method                     | Description                          |\n",
    "| -------------------------- | ------------------------------------ |\n",
    "| `.find_next_sibling()`     | Next tag at the same level           |\n",
    "| `.find_previous_sibling()` | Previous tag at the same level       |\n",
    "| `.find_parent()`           | Immediate parent tag                 |\n",
    "| `.find_next()`             | Next tag in document order           |\n",
    "| `.find_all_next()`         | All following tags in document order |\n",
    "\n",
    "Use this when:\n",
    "\n",
    "* Elements don’t have unique selectors\n",
    "* Structure is consistent but not labeled\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Crawling vs Scraping – Quick Recap\n",
    "\n",
    "| Feature         | Web Crawling                | Web Scraping                        |\n",
    "|-----------------|-----------------------------|-------------------------------------|\n",
    "| Purpose         | Find and index web pages    | Extract data from web pages         |\n",
    "| Output          | List of URLs                | Structured data (text, tables, etc) |\n",
    "| Analogy         | Library scout               | Researcher/data collector           |\n",
    "| Tools           | Scrapy, Nutch               | BeautifulSoup, Selenium, Scrapy     |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91306526",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
