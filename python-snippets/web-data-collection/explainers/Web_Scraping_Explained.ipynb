{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e571ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Web Scraping\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites. Unlike web crawling, which focuses on discovering and listing URLs, web scraping involves **accessing the content within each page and pulling out specific pieces of information** — such as titles, prices, reviews, dates, or any structured content.\n",
    "\n",
    "\n",
    "\n",
    "Web scraping typically involves:\n",
    "1. **Sending HTTP requests** to web pages\n",
    "2. **Parsing the HTML** of the response\n",
    "3. **Extracting target data** using selectors (CSS, XPath, etc.)\n",
    "4. **Structuring and storing the data** in formats like JSON, CSV, or databases\n",
    "\n",
    "Scraping can be done using tools like Python (with libraries such as Scrapy, BeautifulSoup, Selenium)\n",
    "> Note: Always check a website’s Terms of Service and robots.txt file before scraping. Respect ethical and legal boundaries when collecting data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed8e46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Common Use Cases of Web Scraping\n",
    "\n",
    "- **Price Monitoring**  \n",
    "  Websites that track prices across e-commerce sites use scrapers to collect prices daily or hourly.\n",
    "\n",
    "- **Job Aggregators**  \n",
    "  Platforms like Indeed or Glassdoor scrape job listings from multiple company websites.\n",
    "\n",
    "- **Market Research**  \n",
    "  Scrapers collect customer reviews, product specs, or competitor data to support business decisions.\n",
    "\n",
    "- **Academic or Data Projects**  \n",
    "  Researchers and students scrape data for machine learning, NLP, or data analysis tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Real-Life Analogy: The Data Collector\n",
    "\n",
    "Let’s go back to our **library analogy**.\n",
    "\n",
    "- A **web scraper** is like a **researcher** who walks up to the books listed by the scout (crawler).\n",
    "- This researcher **opens the books**, **reads specific pages**, and **copies down key facts** — like author names, chapters, or quotes.\n",
    "- So, while the crawler builds the roadmap, the scraper collects the actual **content**.\n",
    "\n",
    "---\n",
    "\n",
    "### Popular Tools for Web Scraping\n",
    "\n",
    "| Tool                        | Category                | Notes                                             |\n",
    "| --------------------------- | ----------------------- | ------------------------------------------------ |\n",
    "| **BeautifulSoup**           | Scraper                 | Great for parsing HTML and extracting data        |\n",
    "| **Selenium**                | Scraper                 | Handles JavaScript-rendered pages with automation |\n",
    "| **Scrapy**                  | Crawler + Scraper       | End-to-end framework for crawling and scraping    |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Crawling vs Scraping – Quick Recap\n",
    "\n",
    "| Feature         | Web Crawling                | Web Scraping                        |\n",
    "|-----------------|-----------------------------|-------------------------------------|\n",
    "| Purpose         | Find and index web pages    | Extract data from web pages         |\n",
    "| Output          | List of URLs                | Structured data (text, tables, etc) |\n",
    "| Analogy         | Library scout               | Researcher/data collector           |\n",
    "| Tools           | Scrapy, Nutch               | BeautifulSoup, Selenium, Scrapy     |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91306526",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### When They Work Together\n",
    "\n",
    "Often, we use **both crawling and scraping** in the same project:\n",
    "1. The crawler discovers relevant pages.\n",
    "2. The scraper extracts data from those pages.\n",
    "\n",
    "For example, a real estate bot may first crawl all listings on a site, then scrape each one for price, size, and location.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
